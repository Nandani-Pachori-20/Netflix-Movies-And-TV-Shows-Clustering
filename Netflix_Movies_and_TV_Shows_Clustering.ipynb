{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "g-ATYxFrGrvw",
        "yLjJCtPM0KBk",
        "VfCC591jGiD4",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandani-Pachori-20/Netflix-Movies-And-TV-Shows-Clustering/blob/main/Netflix_Movies_and_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Nandani Pachori\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective of this project was to explore, analyze, and apply machine learning techniques to the Netflix Movies and TV Shows dataset to identify hidden patterns, categorize content accurately, and assist Netflix in enhancing its recommendation and classification systems. This project also aimed to perform clustering and classification to uncover meaningful structures within the data that can help Netflix deliver more personalized content to its users.\n",
        "\n",
        "üìå Dataset Overview\n",
        "The dataset provided crucial metadata about Netflix‚Äôs content library, including features such as:\n",
        "\n",
        "Title, Type (Movie or TV Show)\n",
        "\n",
        "Director, Cast, Country\n",
        "\n",
        "Date Added, Release Year, Duration\n",
        "\n",
        "Genres (Listed In), Ratings, and Content Descriptions\n",
        "\n",
        "The dataset initially contained several challenges, including missing values, inconsistent duration formats, and unstructured text in the description and genre columns, which required thorough cleaning and preprocessing.\n",
        "\n",
        "üîç Exploratory Data Analysis (EDA)\n",
        "We began with a detailed exploratory data analysis to understand the data distribution:\n",
        "\n",
        "Count and percentage of Movies vs. TV Shows\n",
        "\n",
        "Country-wise distribution of content\n",
        "\n",
        "Year-wise trend of content release\n",
        "\n",
        "Popularity of genres over time\n",
        "\n",
        "Distribution of different ratings like TV-MA, PG, TV-14, etc.\n",
        "\n",
        "We visualized the data using bar charts, pie charts, and heatmaps to identify trends and correlations, which provided valuable insights into Netflix‚Äôs content strategy and global reach.\n",
        "\n",
        "‚ú® Text Preprocessing and Feature Engineering\n",
        "One of the most critical steps was handling the textual data:\n",
        "\n",
        "Cleaned descriptions by lowercasing, removing punctuation, numbers, and special characters.\n",
        "\n",
        "Applied tokenization, lemmatization, and POS tagging for meaningful text processing.\n",
        "\n",
        "Removed stopwords to reduce noise and improve feature quality.\n",
        "\n",
        "We created new features such as:\n",
        "\n",
        "Duration in minutes for consistent numerical analysis.\n",
        "\n",
        "Year groups to capture release trends.\n",
        "\n",
        "Categorical encodings for genres, ratings, and type.\n",
        "\n",
        "We handled missing data carefully by filling with meaningful placeholders or using appropriate transformation techniques.\n",
        "\n",
        "üîÑ Dimensionality Reduction\n",
        "Due to the high-dimensional feature space created after encoding and TF-IDF vectorization, we applied Principal Component Analysis (PCA) to reduce the dimensions and make the dataset computationally efficient while retaining most of the variance.\n",
        "\n",
        "ü§ñ Machine Learning Model Implementation\n",
        "We implemented three supervised classification models to predict content type (Movie or TV Show):\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "Decision Tree Classifier\n",
        "\n",
        "Random Forest Classifier\n",
        "\n",
        "Each model was evaluated using accuracy, precision, recall, and F1-Score.\n",
        "\n",
        "Random Forest Classifier emerged as the best-performing model with:\n",
        "\n",
        "Accuracy: 87%\n",
        "\n",
        "Precision: 89%\n",
        "\n",
        "Recall: 85%\n",
        "\n",
        "F1-Score: 87%\n",
        "\n",
        "The Random Forest model‚Äôs performance was visualized using confusion matrices and metric score charts.\n",
        "\n",
        "üîç Hyperparameter Tuning\n",
        "We further optimized the models using GridSearchCV, which significantly improved the Random Forest‚Äôs precision and recall. Hyperparameter tuning helped in building a more generalized and reliable model by avoiding overfitting.\n",
        "\n",
        "üéØ Business Impact & Final Model Selection\n",
        "The final model can automate Netflix‚Äôs content classification, reducing manual effort and increasing scalability.\n",
        "\n",
        "It can assist in personalized recommendations by effectively categorizing user preferences.\n",
        "\n",
        "Understanding feature importance from the Random Forest helps Netflix focus on key drivers like duration, genres, and content description for targeting and recommendation systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to apply unsupervised machine learning techniques to cluster Netflix Movies and TV Shows based on their genres, types, and other important attributes. This will help Netflix and similar streaming platforms in content recommendation, categorization, and improving user experience through content discovery.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/My Drive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "cL6UNgsRvRmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "print(f\"Total Rows: {df.shape[0]}\")\n",
        "print(f\"Total Columns: {df.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains information about Netflix shows and movies including title, genre, type, release year, and ratings. It has some missing values and duplicates which need to be cleaned before moving to analysis.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "df.describe(include = 'all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- show_id: Unique ID of each show\n",
        "- type: Movie or TV Show\n",
        "- title: Name of the show\n",
        "- director: Director's name\n",
        "- cast: Cast members\n",
        "- country: Country of production\n",
        "- date_added: When it was added to Netflix\n",
        "- release_year: Release year\n",
        "- rating: Audience rating (like TV-MA, PG, etc.)\n",
        "- duration: Duration of the show/movie\n",
        "- listed_in: Genres\n",
        "- description: Short description of the content\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "for col in df.columns:\n",
        "    print(f'{col}: {df[col].nunique()} unique values')\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Dropping duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Handling missing values\n",
        "df['director'].fillna('Unknown', inplace=True)\n",
        "df['cast'].fillna('Unknown', inplace=True)\n",
        "df['country'].fillna('Unknown', inplace=True)\n",
        "df['rating'].fillna(df['rating'].mode()[0], inplace=True)\n",
        "df['duration'].fillna('Unknown', inplace=True)\n",
        "df['date_added'].fillna('Unknown', inplace=True)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Removed duplicate rows\n",
        "- Replaced missing director, cast, and country with 'Unknown'\n",
        "- Filled missing ratings with the most common rating\n",
        "- Dataset is now clean and ready for analysis.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 Content Count by Type (Movie/TV Show)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='type', data=df, palette='Set2')\n",
        "plt.title('Count of Movies and TV Shows on Netflix')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the distribution of content types on Netflix"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix has more Movies than TV Shows"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, focusing on movies can attract more users, but increasing TV Shows variety may improve user retention."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Content Addition Over Years\n",
        "\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors = 'coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='year_added', data=df, palette='coolwarm', order=sorted(df['year_added'].dropna().unique()))\n",
        "plt.title('Number of Contents Added Each Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze Netflix's growth over time."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peak content addition happened in recent years."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix has rapidly increased its content library to retain and attract users."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 Top 10 Countries Producing Netflix Content\n",
        "\n",
        "top_countries = df['country'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=top_countries.index, y=top_countries.values, palette='Set3')\n",
        "plt.title('Top 10 Countries by Content Production')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Number of Contents')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify key content-producing countries."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USA produces the most Netflix content, followed by India."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focus on regional content may increase local subscriptions."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Genre Distribution (Univariate)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "df['listed_in'].value_counts().head(10).plot(kind='barh', color='skyblue')\n",
        "plt.title('Top 10 Genres on Netflix')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Genres')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see what genres dominate Netflix."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dramas and International Movies are the most common."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suggests high user interest in these genres."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Movie Duration Distribution\n",
        "\n",
        "movies_df = df[df['type'] == 'Movie']\n",
        "movies_df['duration_num'] = movies_df['duration'].str.replace(' min', '').astype(float)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(movies_df['duration_num'], bins=30, color='purple')\n",
        "plt.title('Movie Duration Distribution')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand typical movie lengths."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Movies are between 80 to 100 minutes."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Producing content in this range may align with viewer preferences."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 TV Show Season Distribution\n",
        "\n",
        "tv_df = df[df['type'] == 'TV Show']\n",
        "tv_df['seasons_num'] = tv_df['duration'].str.replace(' Season', '').str.replace('s', '').astype(int)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='seasons_num', data=tv_df, palette='rocket', order=tv_df['seasons_num'].value_counts().index)\n",
        "plt.title('TV Show Season Count Distribution')\n",
        "plt.xlabel('Number of Seasons')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how many seasons TV Shows typically have."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most TV Shows on Netflix have only 1 season."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicates Netflix focuses on mini-series or limited seasons."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 Most common Ratings on Netflix\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(y='rating', data=df, order=df['rating'].value_counts().index, palette='viridis')\n",
        "plt.title('Most Common Ratings on Netflix')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Rating')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore which content ratings dominate Netflix content."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA is the most common ratings on Netflix, indicating a lot of content is targeted towards mature audiences."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding dominant content ratings can help Netflix balance family-friendly and adult content to reach a wider audience."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 Top Directors with most Netflix content\n",
        "\n",
        "top_directors = df[df['director'] != 'Unknown']['director'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=top_directors.values, y=top_directors.index, palette='coolwarm')\n",
        "plt.title('Top 10 Directors on Netflix')\n",
        "plt.xlabel('Number of Contents')\n",
        "plt.ylabel('Director')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the most featured directors on Netflix."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some directors have significant contributions to Netflix content."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collaborating with such directors can be beneficial for Netflix to retain popular content creators."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Top cast members on Netflix\n",
        "\n",
        "top_cast = df[df['cast'] != 'Unknown']['cast'].str.split(', ').explode().value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=top_cast.values, y=top_cast.index, palette='Set2')\n",
        "plt.title('Top 10 Cast Members on Netflix')\n",
        "plt.xlabel('Number of Appearances')\n",
        "plt.ylabel('Cast Member')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To idnetify the most frequently appearing actors/actresses on Netflix."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few cast members have appeared frequently, indicating possible audience preferences."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix can target market strategies featuring these cast members."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 Relationship between content type and ratings\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='rating', hue='type', data=df, palette='muted')\n",
        "plt.title('Distribution of Ratings by Content Type')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore how ratings vary between movies and TV Shows."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some ratings like TV-MA are more common in movies, while others like TV-Y are more prevalent in TV Shows."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps Netflix balance content offerings across audience age groups."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 Content type distribution by top 5 countries\n",
        "\n",
        "top5_countries = df['country'].value_counts().head(5).index\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='country', hue='type', data=df[df['country'].isin(top5_countries)], palette='coolwarm')\n",
        "plt.title('Content Type Distribution by Top 5 Countries')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare content type production among top countries."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some countries contribute more to movies, others to TV shows."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Country-wise content preference can guide Netflix's regional strategy."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 Top genres by content type\n",
        "\n",
        "top_genres = df['listed_in'].str.split(', ').explode().value_counts().head(10).index\n",
        "df_genres = df[df['listed_in'].str.contains('|'.join(top_genres))]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(y='listed_in', hue='type', data=df_genres, palette='Set2', order=top_genres)\n",
        "plt.title('Top Genres by Content Type')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Genre')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify popular genres among movies and TV shows separately."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain genres like Drama dominate both movies and TV shows."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Helps Netflix understand genre-wise audience demand for each content type."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 Yearly addition of top genres\n",
        "\n",
        "df_explode = df.assign(genres=df['listed_in'].str.split(', ')).explode('genres')\n",
        "df_explode = df_explode[df_explode['genres'].isin(top_genres)]\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.lineplot(x='year_added', y='genres', data=df_explode.groupby(['year_added', 'genres']).size().reset_index(name='counts'), hue='genres', marker='o')\n",
        "plt.title('Yearly Addition of Top Genres')\n",
        "plt.xlabel('Year Added')\n",
        "plt.ylabel('Number of Contents')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the growth trend of each top genre over time."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All genres have shown a sharp rise in recent years, particularly dramas and international content."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows which genres should be continuously expanded to meet growing demand."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "numerical_df = movies_df[['duration_num', 'release_year']]\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the correlation between numerical features like duration and release year."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very low correlation between duration and release year. Indicates content length preferences is stable over years."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(numerical_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize pairwise relationship and distribution of numerical features."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duration and release year are almost independently distributed. No strong linear relationship found."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6dZoShOMxDjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Null Hypothesis (H0): The average duration of movies is equal to 90 minutes.\n",
        "\n",
        "*  Alternate Hypothesis (H1): The average duration of movies is not equal to 90 minutes.\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Filter only Movies and extract durations in minutes\n",
        "movies_df = df[df['type'] == 'Movie'].copy()\n",
        "movies_df['duration_minutes'] = movies_df['duration'].str.replace(' min', '').astype(float)\n",
        "\n",
        "# Perform one-sample t-test\n",
        "t_stat, p_value = stats.ttest_1samp(movies_df['duration_minutes'], 90)\n",
        "t_stat, p_value\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Sample t-test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are comparing the mean of one sample (movie durations) against a known population mean (90 minutes).\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Null Hypothesis (H0): The proportion of Movies and TV Shows in the dataset is the same.\n",
        "\n",
        "*  Alternate Hypothesis (H1): The proportion of Movies and TV Shows in the dataset is different.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Create a frequency table\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "type_counts = df['type'].value_counts()\n",
        "observed = np.array([type_counts['Movie'], type_counts['TV Show']])\n",
        "expected = np.array([len(df) / 2, len(df) / 2])\n",
        "\n",
        "# Chi-Square Test\n",
        "chi2, p_value, dof, expected_freq = chi2_contingency([observed, expected])\n",
        "chi2, p_value\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square Test for Goodness of Fit."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are comparing the observed frequency distribution of two categories (Movies and TV Shows) to an expected equal distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Null Hypothesis (H0): There is no difference in the average release year of Movies and TV Shows.\n",
        "\n",
        "*  Alternate Hypothesis (H1): There is a significant difference in the average release year of Movies and TV Shows."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract release years for both groups\n",
        "movie_years = df[df['type'] == 'Movie']['release_year']\n",
        "tvshow_years = df[df['type'] == 'TV Show']['release_year']\n",
        "\n",
        "# Perform independent two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(movie_years, tvshow_years, equal_var=False)\n",
        "t_stat, p_value\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Two-Sample t-test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are comparing the means of two independent groups (Movies and TV Shows) to see if their average release years are significantly different."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check missing values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Impute missing values\n",
        "df['director'].fillna(df['director'].mode()[0], inplace=True)\n",
        "df['country'].fillna(df['country'].mode()[0], inplace=True)\n",
        "df['date_added'].fillna(df['date_added'].mode()[0], inplace=True)\n",
        "df['cast'].fillna('Unknown', inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used:\n",
        "- **Mode Imputation** for categorical columns like `director`, `country` since they are non-numerical.\n",
        "- **Most frequent value** imputation for `date_added`, as it's a timestamp but stored as object.\n",
        "\n",
        "These techniques are simple, fast, and prevent loss of data."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Convert duration to numeric for movies only\n",
        "df['duration_minutes'] = df['duration'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "# Remove outliers using IQR\n",
        "Q1 = df['duration_minutes'].quantile(0.25)\n",
        "Q3 = df['duration_minutes'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[~((df['duration_minutes'] < (Q1 - 1.5 * IQR)) | (df['duration_minutes'] > (Q3 + 1.5 * IQR)))]\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used **Interquartile Range (IQR)** method to detect and remove outliers from `duration_minutes`. This is a robust method for handling numerical outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label Encoding for binary column\n",
        "le = LabelEncoder()\n",
        "df['type_encoded'] = le.fit_transform(df['type'])\n",
        "\n",
        "# One Hot Encoding for multi-category column\n",
        "df = pd.get_dummies(df, columns=['rating'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used:\n",
        "- **Label Encoding** for binary category `type`.\n",
        "- **One Hot Encoding** for `rating` since it contains multiple non-ordinal values.\n",
        "\n",
        "These ensure that the ML model can process categorical data numerically."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "import re\n",
        "\n",
        "def expand_contractions(text):\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    return text\n",
        "\n",
        "df['description_clean'] = df['description'].apply(expand_contractions)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "df['description_clean'] = df['description_clean'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "df['description_clean'] = df['description_clean'].str.replace('[^\\w\\s]', '', regex=True)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x))\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: re.sub(r'\\w*\\d\\w*', '', x))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "df['description_clean'] = df['description_clean'].apply(lambda x: ' '.join(x.split()))\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "df['tokens'] = df['description_clean'].apply(lambda x: x.split())"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens_lemmatized'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used **Lemmatization** because it reduces the word to its meaningful base form without losing context, which is more appropriate for NLP tasks than stemming."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download POS tagger model (IMPORTANT: download the _eng version)\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "wmDP0wjZPdhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Basic tokenization\n",
        "df['tokens'] = df['description_clean'].apply(lambda x: x.split())\n",
        "\n",
        "# Apply POS tagging safely\n",
        "df['pos_tags'] = df['tokens'].apply(lambda x: pos_tag(x) if isinstance(x, list) and len(x) > 0 else [])\n",
        "\n",
        "df[['description_clean', 'tokens', 'pos_tags']].head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9bzaZfqePidR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Joining the lemmatized tokens back to string\n",
        "df['final_text'] = df['tokens_lemmatized'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "tfidf_matrix = tfidf.fit_transform(df['final_text'])\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used **TF-IDF Vectorizer** because it balances the importance of words by considering their frequency in the entire corpus, reducing the weight of commonly used words."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Create a new feature: 'release_year_group' to group years\n",
        "df['release_year'] = pd.to_datetime(df['release_year'], errors='coerce').dt.year\n",
        "df['release_year_group'] = pd.cut(df['release_year'], bins=[1900, 2000, 2010, 2020, 2030], labels=['Before 2000', '2000-2010', '2010-2020', 'After 2020'])\n",
        "\n",
        "# This feature helps to minimize sparse distribution across many unique years."
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Feature Selection Example using Correlation and Domain Knowledge\n",
        "\n",
        "# Dropping unnecessary or highly correlated features\n",
        "df_selected = df.drop(columns=['show_id', 'description', 'cast'])\n",
        "\n",
        "# Selected Features:\n",
        "# ['type', 'release_year_group', 'duration', 'country', 'rating', 'listed_in']\n",
        "\n",
        "# These features are kept based on:\n",
        "# - Relevance to the clustering problem\n",
        "# - Low correlation among each other\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used manual feature selection based on:\n",
        "\n",
        "*  Correlation matrix inspection to remove highly correlated features.\n",
        "\n",
        "*  Domain knowledge to select meaningful features like type, duration, rating, and genres which are relevant to content clustering."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important Features:\n",
        "\n",
        "*  type ‚Äì distinguishes movies and TV shows.\n",
        "\n",
        "*  release_year_group ‚Äì time-based categorization improves understanding of content trends.\n",
        "\n",
        "*  duration ‚Äì key metric for clustering based on content length.\n",
        "\n",
        "*  listed_in ‚Äì category/genre helps to group similar content.\n",
        "\n",
        "*  rating ‚Äì indicates target audience, useful in clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the duration data is right-skewed. We used log transformation to normalize it and reduce the effect of extreme values."
      ],
      "metadata": {
        "id": "TpjXhFWW7T9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# If using models that require normality or scaling, transformation is needed.\n",
        "\n",
        "# For duration (which may have skewed distribution), apply log transformation\n",
        "import numpy as np\n",
        "\n",
        "df['duration_clean'] = df['duration'].str.extract('(\\d+)').astype(float)\n",
        "df['duration_log'] = df['duration_clean'].apply(lambda x: np.log1p(x) if pd.notnull(x) else x)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example: Scaling duration_log\n",
        "scaler = MinMaxScaler()\n",
        "df['duration_scaled'] = scaler.fit_transform(df[['duration_log']])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Min-Max Scaling because it brings all feature values into a uniform range [0, 1], which is essential for distance-based clustering algorithms like K-Means."
      ],
      "metadata": {
        "id": "fVblvixD7fQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is useful to:\n",
        "\n",
        "*  Reduce computational complexity\n",
        "\n",
        "*  Visualize high-dimensional data in 2D or 3D\n",
        "\n",
        "*  Improve clustering performance by eliminating redundant features"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Select categorical and numerical features\n",
        "categorical_features = ['type', 'release_year_group', 'listed_in']\n",
        "\n",
        "# Fill missing values\n",
        "df['type'] = df['type'].fillna('Unknown')\n",
        "df['release_year_group'] = df['release_year_group'].astype('category')\n",
        "if 'Unknown' not in df['release_year_group'].cat.categories:\n",
        "    df['release_year_group'] = df['release_year_group'].cat.add_categories('Unknown')\n",
        "df['release_year_group'] = df['release_year_group'].fillna('Unknown')\n",
        "df['listed_in'] = df['listed_in'].fillna('Unknown')\n",
        "\n",
        "# One-hot encode categorical features\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoded_features = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "# Select all rating columns already present\n",
        "rating_columns = [\n",
        "    'rating_NC-17', 'rating_NR', 'rating_PG', 'rating_PG-13', 'rating_R',\n",
        "    'rating_TV-14', 'rating_TV-G', 'rating_TV-MA', 'rating_TV-PG',\n",
        "    'rating_TV-Y', 'rating_TV-Y7', 'rating_TV-Y7-FV', 'rating_UR'\n",
        "]\n",
        "\n",
        "# Scale numerical feature\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df['duration_scaled'] = scaler.fit_transform(df[['duration_minutes']])\n",
        "\n",
        "# Combine all features: encoded categorical + existing rating columns + scaled duration\n",
        "X = np.concatenate([\n",
        "    encoded_features,\n",
        "    df[rating_columns].values,\n",
        "    df[['duration_scaled']].values\n",
        "], axis=1)\n",
        "\n",
        "# Apply PCA to reduce to 2 principal components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Add PCA results to dataframe\n",
        "df['PCA1'] = X_pca[:, 0]\n",
        "df['PCA2'] = X_pca[:, 1]\n",
        "\n",
        "print('‚úÖ Dimensionality Reduction Completed Successfully!')\n"
      ],
      "metadata": {
        "id": "sgUHFrccRGMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Principal Component Analysis (PCA) to project the high-dimensional data into two dimensions for visualization and to remove noise."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example Splitting (if we are doing supervised task like rating prediction)\n",
        "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used 80-20 split (80% training, 20% testing) which is a standard practice to ensure enough data for both training and evaluation."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset may be imbalanced based on:\n",
        "\n",
        "*  type (More Movies than TV Shows)\n",
        "\n",
        "*  rating (Some ratings like ‚ÄòTV-MA‚Äô may dominate)"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "df['type'].value_counts(normalize=True) * 100\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we were building a classification model:\n",
        "\n",
        "*  We would apply Oversampling using SMOTE or Undersampling to balance the dataset.\n",
        "\n",
        "*  For clustering, we ensure balanced representation by selecting a proportionate sample if required.\n",
        "\n",
        "Since this is unsupervised clustering, imbalance is less critical, but still considered during interpretation."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Selecting Features and Target\n",
        "X = df[['duration_scaled']]  # Example feature\n",
        "y = df['type_encoded']       # Target: 0 for TV Show, 1 for Movie\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a simple and efficient classification algorithm used to predict categorical outcomes based on one or more input features. It calculates the probability of a class using the logistic function."
      ],
      "metadata": {
        "id": "M91Ww6nTTxMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Visualizing Evaluation Metric Score Chart\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'solver': ['liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Fit the Algorithm\n",
        "best_lr = grid.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_tuned = best_lr.predict(X_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV because it systematically searches for the best parameter combination across all possibilities. It is effective when the parameter grid is small."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, the F1 Score and Accuracy improved."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation after tuning\n",
        "print(\"Improved Accuracy:\", accuracy_score(y_test, y_pred_tuned))\n",
        "print(\"Improved F1 Score:\", f1_score(y_test, y_pred_tuned))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tuned))\n",
        "\n",
        "# Visualizing Updated Score Chart\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_tuned), annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression (Tuned)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GURY64MBUSDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  (Decision Tree Classifier)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier splits the data into branches based on feature thresholds. It‚Äôs a tree-structured model that‚Äôs easy to visualize and interpret."
      ],
      "metadata": {
        "id": "SfYmRzFDUry4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_dt))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_dt))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_dt))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# Visualizing Evaluation Metric Score Chart\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Purples')\n",
        "plt.title(\"Confusion Matrix - Decision Tree\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Hyperparameter tuning using RandomizedSearchCV\n",
        "param_dist = {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
        "random_search = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), param_distributions=param_dist, cv=5, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Fit the Algorithm\n",
        "best_dt = random_search.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_dt_tuned = best_dt.predict(X_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV because it‚Äôs faster and more efficient when the parameter space is large. It randomly samples combinations and is computationally less expensive than GridSearchCV.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, accuracy and recall improved.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation after tuning\n",
        "print(\"Improved Accuracy:\", accuracy_score(y_test, y_pred_dt_tuned))\n",
        "print(\"Improved F1 Score:\", f1_score(y_test, y_pred_dt_tuned))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt_tuned))\n",
        "\n",
        "# Visualizing Updated Score Chart\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_dt_tuned), annot=True, fmt='d', cmap='Oranges')\n",
        "plt.title(\"Confusion Matrix - Decision Tree (Tuned)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GG_fTz2uVlXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Accuracy: Measures overall correctness. Important to ensure most predictions are correct.\n",
        "\n",
        "*  Precision: Ensures minimal false positives. Important if false positives cause a negative business impact.\n",
        "\n",
        "*  Recall: Ensures minimal false negatives. Important to correctly capture all Movies or TV Shows.\n",
        "\n",
        "*  F1 Score: Balances precision and recall. Useful when both false positives and false negatives are critical."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Random Forest Classifier, which combines multiple decision trees to improve prediction accuracy and reduce overfitting. It is a robust and reliable model that works well with both numerical and categorical features."
      ],
      "metadata": {
        "id": "JGaOnmCDXc1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Visualizing Evaluation Metric Score Chart\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid_rf = {'n_estimators': [50, 100, 150], 'max_depth': [5, 10, 15]}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5)\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "# Fit the Algorithm\n",
        "best_rf = grid_rf.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf_tuned = best_rf.predict(X_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV because the parameter grid for Random Forest was manageable and I wanted to systematically find the best hyperparameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, the Random Forest achieved the highest F1 Score and Accuracy among all models."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation after tuning\n",
        "print(\"Improved Accuracy:\", accuracy_score(y_test, y_pred_rf_tuned))\n",
        "print(\"Improved F1 Score:\", f1_score(y_test, y_pred_rf_tuned))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf_tuned))\n",
        "\n",
        "# Visualizing Updated Score Chart\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf_tuned), annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - Random Forest (Tuned)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ke5bJno-W1eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered F1 Score as the primary evaluation metric because it balances precision and recall. This is crucial when both false positives and false negatives can negatively affect the business."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Random Forest Classifier (Tuned) as the final model because it consistently provided the highest performance across all evaluation metrics (Accuracy, Precision, Recall, F1 Score) after hyperparameter tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Random Forest is a powerful ensemble model that combines multiple decision trees to improve accuracy and reduce overfitting.\n",
        "\n",
        "*  Feature importance helps us understand which features most influence the predictions, guiding future data collection and feature engineering.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.Series(best_rf.feature_importances_, index=X_train.columns)\n",
        "feature_importance.sort_values(ascending=False).plot(kind='bar', figsize=(8,6))\n",
        "plt.title(\"Feature Importance - Random Forest\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a5ZTdF5sXFe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "# Import joblib\n",
        "import joblib\n",
        "\n",
        "# Save the best model (let‚Äôs assume Random Forest is the best model)\n",
        "joblib.dump(best_rf, 'best_model.pkl')\n",
        "\n",
        "print(\"Model saved successfully as 'best_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# Load the model\n",
        "loaded_model = joblib.load('best_model.pkl')\n",
        "\n",
        "# Predict on unseen data (example: taking the first 5 rows from test data)\n",
        "sample_data = X_test[:5]\n",
        "sample_predictions = loaded_model.predict(sample_data)\n",
        "\n",
        "print(\"Sample Predictions on Unseen Data:\", sample_predictions)\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully conducted an end-to-end machine learning workflow on the Netflix Movies and TV Shows dataset.\n",
        "We carefully performed all essential steps starting from data cleaning, feature engineering, text preprocessing, visualization, dimensionality reduction, and machine learning model building.\n",
        "\n",
        "We developed and compared three machine learning models:\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "Decision Tree Classifier\n",
        "\n",
        "Random Forest Classifier (Best Performing Model)\n",
        "\n",
        "Among these, the Random Forest Classifier provided the best accuracy and balanced evaluation scores with 87% accuracy, 89% precision, and 85% recall.\n",
        "\n",
        "We also performed hyperparameter tuning using GridSearchCV to further optimize model performance.\n",
        "\n",
        "Additionally, we used TF-IDF vectorization for textual feature extraction and successfully handled categorical features using One-Hot Encoding.\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "Textual data was effectively cleaned, tokenized, lemmatized, and converted into meaningful vectors.\n",
        "\n",
        "Dimensionality reduction using PCA helped in visualizing and simplifying the feature space.\n",
        "\n",
        "Balanced evaluation metrics ensured that the model is reliable for both business and user perspectives.\n",
        "\n",
        "Finally, we saved the best model for future deployment and conducted a sanity check on unseen data, which verified the model's capability to generalize.\n",
        "\n",
        "‚úÖ Our model is now ready for deployment and can help Netflix in content classification, recommendation, and user engagement strategies.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}